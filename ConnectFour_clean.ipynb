{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()  # Clears cached memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2\n",
      "2.5.1+cu121\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#mixed precision\n",
    "#comments for CNN \n",
    "#readme\n",
    "#chess\n",
    "#varying the parameters of the model to optimize the performance\n",
    "\n",
    "import numpy as np\n",
    "print(np.__version__)\n",
    "\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import random\n",
    "import math\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from tqdm import trange\n",
    "import torch\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ConnectFour:\n",
    "    def __init__(self):\n",
    "        self.row_count = 6\n",
    "        self.column_count = 7\n",
    "        self.action_size = self.column_count\n",
    "        self.in_a_row = 4\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"ConnectFour\"\n",
    "        \n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "    def get_next_state(self, state, action, player):\n",
    "        row = np.max(np.where(state[:, action] == 0))\n",
    "        state[row, action] = player\n",
    "        return state\n",
    "    \n",
    "    def get_valid_moves(self, state):\n",
    "        return (state[0] == 0).astype(np.uint8)\n",
    "    \n",
    "    def check_win(self, state, action): #state is the current state and action is the action taken by the opponent\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = np.min(np.where(state[:, action] != 0))\n",
    "        column = action\n",
    "        player = state[row][column]\n",
    "\n",
    "        def count(offset_row, offset_column):\n",
    "            for i in range(1, self.in_a_row):\n",
    "                r = row + offset_row * i\n",
    "                c = action + offset_column * i\n",
    "                if (\n",
    "                    r < 0 \n",
    "                    or r >= self.row_count\n",
    "                    or c < 0 \n",
    "                    or c >= self.column_count\n",
    "                    or state[r][c] != player\n",
    "                ):\n",
    "                    return i - 1\n",
    "            return self.in_a_row - 1\n",
    "\n",
    "        return (\n",
    "            count(1, 0) >= self.in_a_row - 1 # vertical\n",
    "            or (count(0, 1) + count(0, -1)) >= self.in_a_row - 1 # horizontal\n",
    "            or (count(1, 1) + count(-1, -1)) >= self.in_a_row - 1 # top left diagonal\n",
    "            or (count(1, -1) + count(-1, 1)) >= self.in_a_row - 1 # top right diagonal\n",
    "        )\n",
    "    \n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        return -1*player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -1*value\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        if len(state.shape) == 3:\n",
    "            encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
    "        \n",
    "        return encoded_state\n",
    "\n",
    "    def computer_play(self):\n",
    "        return  random.randint(0,self.column_count-1)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self,game,num_resBlocks,num_hidden, in_channels):\n",
    "        super().__init__()\n",
    "        #input dimension is : 100x3x6x7 \n",
    "        #kernel = convolutional matrix \n",
    "        #num_hidden = number of filters\n",
    "        #kernel_size 3x3\n",
    "        self.num_resBlocks = num_resBlocks\n",
    "        self.in_channels = in_channels #this is the channel size of the matrice, the reason this is  3 because the cases where player 1 plays , where player 2 plays and nobody plays is represented by 3 different layers where each is represented by 1 and the rest being 0 for connect 4. \n",
    "        self.out_channels = num_hidden # channel size of the output not the width and length  \n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels,self.out_channels,kernel_size=3,padding=1), #3 6 7 -> num_hidden 6 7\n",
    "            nn.BatchNorm2d(self.out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.BackBone = nn.ModuleList([ResBlock(self.out_channels) for i in range(self.num_resBlocks)] ) #We use modulelist instead of sequential because it allows us to use skip connections \n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Conv2d(self.out_channels, 3, kernel_size=3, padding=1), #DONT KNOW WHY 32 yet may be empirical\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3*game.row_count*game.column_count, 1),\n",
    "            nn.Tanh()  #value is between -1 and 1\n",
    "        )\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Conv2d(self.out_channels, 32, kernel_size=3, padding=1), #DONT KNOW WHY 32 yet \n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32*game.row_count*game.column_count, game.action_size), #the action size for each game may be different \n",
    "            \n",
    "        )\n",
    "        self.to(device)\n",
    "   \n",
    "\n",
    "    def forward(self,state): \n",
    "        \n",
    "        x = self.startBlock(state)\n",
    "        for block in self.BackBone:\n",
    "            x = block(x)  # Apply each ResBlock manually\n",
    "        policy = self.policy_head(x)\n",
    "        value = self.value_head(x)\n",
    "        return policy, value\n",
    "\n",
    "        \n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self,out_channels):\n",
    "        super().__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.Conv2d_1 = nn.Conv2d(self.out_channels,self.out_channels,kernel_size=3,padding=1)\n",
    "        self.BatchNorm_1 = nn.BatchNorm2d(self.out_channels)\n",
    "        self.Conv2d_2 = nn.Conv2d(self.out_channels,self.out_channels,kernel_size=3,padding=1)\n",
    "        self.BatchNorm_2 = nn.BatchNorm2d(self.out_channels)\n",
    "\n",
    "    def forward(self,x):\n",
    "        temp = x\n",
    "        x = self.BatchNorm_2(self.Conv2d_2(F.relu(self.BatchNorm_1(self.Conv2d_1(x)))))\n",
    "        x = temp + x\n",
    "        x = F.relu(x)\n",
    "        return x \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "     def __init__(self,state,args,transition_action, player,game):\n",
    "          self.state = state \n",
    "          self.nb_visits = 0\n",
    "          self.value  = 0 \n",
    "          self.prior_proba = 0\n",
    "          self.children = []\n",
    "          self.parent  = None \n",
    "          self.args = args      \n",
    "          self.transition_action = transition_action\n",
    "          self.player = player \n",
    "          self.game = game\n",
    "\n",
    "\n",
    "     def get_prior_proba(self):\n",
    "          return self.prior_proba\n",
    "     def get_player(self):\n",
    "          return self.player\n",
    "     def get_transition_action(self):\n",
    "          return self.transition_action\n",
    "\n",
    "     def get_visits(self):\n",
    "          return self.nb_visits\n",
    "     def get_state(self):\n",
    "          return self.state\n",
    "     def set_value(self, value ):\n",
    "          self.value = value\n",
    "\n",
    "     def get_value(self):\n",
    "          return self.value\n",
    "     def set_prior_proba(self,proba):\n",
    "          self.prior_proba = proba\n",
    "\n",
    "     def get_children(self):\n",
    "          return self.children\n",
    "\n",
    "     def add_child(self, child_node,transition_action):\n",
    "          self.children.append((child_node,transition_action))\n",
    "          child_node.set_parent(self)\n",
    "          \n",
    "     def set_parent(self, parent_node):\n",
    "          self.parent = parent_node\n",
    "\n",
    "     def get_parent(self):\n",
    "          return self.parent \n",
    "\n",
    "     def calc_PUCT(self):\n",
    "          parent = self.get_parent() \n",
    "          parent_visits = parent.get_visits()\n",
    "          if self.nb_visits == 0:\n",
    "            q_value = 0\n",
    "          else:\n",
    "               q_value = 1 - ((self.value / self.nb_visits) + 1) / 2\n",
    "          res = q_value + self.args['C'] * (math.sqrt(parent_visits) / (self.nb_visits + 1)) *  self.prior_proba\n",
    "          #print(f\"res with nb_visits = \",self.nb_visits, \" and parent_visits = \", parent_visits, \" is \", res)\n",
    "          return res\n",
    "\n",
    "     def choose_best_child(self):\n",
    "          print(\"--------\")\n",
    "          for child in self.children:\n",
    "               print(\"child.nb_visits = \", child[0].get_visits())\n",
    "          if self.children != []:\n",
    "               best_tuple = max(self.children, key=lambda item: item[0].calc_PUCT()) #we choose the best child node based on the highest PUCT value\n",
    "               best_child = best_tuple[0]\n",
    "               best_action = best_tuple[1]\n",
    "\n",
    "               return best_child, best_action\n",
    "          else :\n",
    "              return None, None\n",
    "\n",
    "\n",
    "     def increment_visit(self):\n",
    "          self.nb_visits += 1 \n",
    "\n",
    "     def add_value(self, value):\n",
    "          self.value += value \n",
    "     def selfPlay_policy(self):\n",
    "          action_size = 7\n",
    "          total_visits = sum(child[0].get_visits() for child in self.children) + 1e-8\n",
    "          policy = [0] * action_size\n",
    "\n",
    "          for child_node, _ in self.children:\n",
    "               action = child_node.get_transition_action()\n",
    "               if action is not None:\n",
    "                    # Use += in case there are multiple child nodes for the same action (edge case)\n",
    "                    policy[action] += child_node.get_visits() / total_visits\n",
    "\n",
    "          return policy\n",
    "\n",
    "     def is_fully_expanded(self):\n",
    "          if self.children == []:\n",
    "               return False\n",
    "          else:\n",
    "               return True\n",
    "                    \n",
    "     def expand(self,probas):\n",
    "          for i, proba in enumerate(probas) :  \n",
    "                #either we get all the actions or the all allowed actions I am not sure but the dimensions of the policy returned by the ResNet must equal the number of actions / states \n",
    "                if proba > 0 : \n",
    "                    child_state = self.state.copy()\n",
    "                    child_state = self.game.get_next_state(child_state, i, 1)\n",
    "                    child_state = self.game.change_perspective(child_state, player=-1)#dont forget to change the perspective every time a new node is added\n",
    "                    print(\"child_state in expand   = \", child_state)\n",
    "                    child_node = Node(child_state,self.args,i,self.player*-1,self.game)\n",
    "                    child_node.set_prior_proba(proba)\n",
    "                    child_node.set_parent(self)\n",
    "                    self.add_child(child_node,i)\n",
    "\n",
    "\n",
    "     def backpropagate(self, value):\n",
    "        self.value += value\n",
    "        self.nb_visits += 1\n",
    "        \n",
    "        value = self.game.get_opponent_value(value)\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(value)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 91)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<string>:91\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"\"\" def backprop(self, node, value):\u001b[0m\n\u001b[1;37m                                        ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "\"\"\"class MCTS:\n",
    "    def __init__(self,game,args,model,states):\n",
    "        \n",
    "        self.game =game \n",
    "        self.root_nodes = [Node(state,args,None,1) for state in states ]\n",
    "        self.args = args\n",
    "        self.model=model \n",
    "\n",
    "\n",
    "    def parallel_selection(self):\n",
    "        # In MCTS.parallel_selection\n",
    "        sample_state = self.root_nodes[0].get_state() if self.root_nodes else None\n",
    "        nodes_and_actions = []\n",
    "        nodes_and_actions = [(self.selection(node)) for node in self.root_nodes] #we select the best child for each root node\n",
    "\n",
    "        return nodes_and_actions\n",
    "    \n",
    "    def selection(self,root):\n",
    "        current_node = root \n",
    "        if current_node.get_children() == []:\n",
    "            return current_node, None\n",
    "\n",
    "        while True:\n",
    "                \n",
    "                next_node, action = current_node.choose_best_child()\n",
    "                \n",
    "                if next_node is None:\n",
    "                    return current_node, action  \n",
    "                \n",
    "                current_node = next_node\n",
    "                \n",
    "                \n",
    "\n",
    "    #@torch.no_grad() # EXTREMELY IMPORTANT IF NOT THE GPU MEMORY FILLS UP \n",
    "    def expansion(self,nodes_and_actions):\n",
    "        if not nodes_and_actions:\n",
    "            return\n",
    "        \n",
    "        remaining_nodes = []        \n",
    "        for node, action in nodes_and_actions:\n",
    "           value, is_terminal = self.game.get_value_and_terminated(node.get_state(), action)\n",
    "           if is_terminal:\n",
    "               self.backprop(node, value) #we only backpropagate the terminal nodes in the less computationally expensive version of the MCTS\n",
    "           else:\n",
    "               remaining_nodes.append((node, action))\n",
    "        if not remaining_nodes:\n",
    "           return\n",
    "        \n",
    "        states_encoded = np.stack([self.game.get_encoded_state(np.array(node.get_state())) for node, action in remaining_nodes]) #np.stack stacks the arrays along the first axis meaning it creates a single contigous array instead of an array of arrays\n",
    "        states_not_encoded = [np.array(node.get_state()) for node, action in remaining_nodes]\n",
    "        states_tensor = torch.tensor(states_encoded, device=device, dtype=torch.float32)\n",
    "        with torch.no_grad(): \n",
    "            policies, values = self.model.forward(states_tensor)\n",
    "        policies = torch.softmax(policies, axis=1).detach().cpu().numpy()\n",
    "        policies = (1 - self.args['dirichlet_epsilon']) * policies + self.args['dirichlet_epsilon'] \\\n",
    "            * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size)\n",
    "        \n",
    "        valid_moves = np.array([self.game.get_valid_moves(state) for state in states_not_encoded])\n",
    "        policies = policies * valid_moves # Normalize each policy row\n",
    "        policies /= np.sum(policies)\n",
    "        #the dirichlet noise is added to the policy to encourage exploration\n",
    "        \"\"\"\n",
    "\n",
    "\"\"\"for i, node_and_action in enumerate(remaining_nodes):\n",
    "            node, action = node_and_action\n",
    "            node.set_value(values[i])\n",
    "            self.backprop(node, values[i]) #we backpropagate the value of the node to the parent nodes\"\"\" #backprop is done for every evaluation in the original alphazero paper but is computationally expensive removing this saved A LOT of time\n",
    "       \"\"\" for j, policy in enumerate(policies):\n",
    "            state = states_not_encoded[j]\n",
    "            node, action = remaining_nodes[j]\n",
    "            modified_policy = policies[j]\n",
    "            value = values[j]\n",
    "            self.backprop(node, value) #we backpropagate the value of the node to the parent nodes\n",
    "            for i, proba in enumerate(modified_policy) :  \n",
    "                #either we get all the actions or the all allowed actions I am not sure but the dimensions of the policy returned by the ResNet must equal the number of actions / states \n",
    "                if proba > 0 : \n",
    "                    player = self.game.get_opponent(node.get_player())\n",
    "                    next_state = self.game.get_next_state(state, i, player)\n",
    "                    next_state = self.game.change_perspective(next_state, player) #dont forget to change the perspective every time a new node is added\n",
    "                    child_node = Node(next_state,self.args,i,player)\n",
    "                    child_node.set_prior_proba(proba)\n",
    "                    child_node.set_parent(node)\n",
    "                    child_node.nb_visits = 1\n",
    "                    node.add_child(child_node,i)\"\"\"\n",
    "                    \n",
    "\n",
    "                    \n",
    "\n",
    "        \n",
    "\n",
    "   \"\"\" def backprop(self, node, value):\n",
    "        node.value += value \n",
    "        node.nb_visits += 1\n",
    "        parent = node.get_parent()\n",
    "        if parent != None:\n",
    "            value = self.game.get_opponent_value(value)\n",
    "            self.backprop(parent, value) #using recursion instead of while loop is more efficient\n",
    "        else:\n",
    "            return\n",
    "        \n",
    "\n",
    "    def perform_episode(self):\n",
    "\n",
    "        nodes_and_actions = self.parallel_selection()\n",
    "        self.expansion(nodes_and_actions) #we perform an episode by performing selection, expansion and backpropagation\n",
    "        \n",
    "    def get_possible_actions(self,node):\n",
    "        possible_acitons = self.game.get_valid_moves(node.get_state())\n",
    "        return possible_acitons\n",
    "    \n",
    "    def round_of_selfPlay(self):\n",
    "        #we perform MCTS_iterations number of episodes and return the number of visits for each action which is forms the policy for the state\n",
    "        total_nodes = sum(len(node.get_children()) for node in self.root_nodes)\n",
    "        for iteration in range(self.args[\"MCTS_iterations\"]):\n",
    "            self.perform_episode()\n",
    "\n",
    "\n",
    "        true_probas = [node.selfPlay_policy() for node in self.root_nodes]\n",
    "\n",
    "        return true_probas\"\"\"\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSParallel:\n",
    "\n",
    "    def __init__(self,game,args,model):\n",
    "        self.game =game \n",
    "        self.args = args\n",
    "        self.model=model \n",
    "\n",
    "\n",
    "    @torch.no_grad() # EXTREMELY IMPORTANT IF NOT THE GPU MEMORY FILLS UP\n",
    "    def search(self,states,spGames,player):\n",
    "            \n",
    "\n",
    "        policies, values =  self.model.forward(torch.tensor(self.game.get_encoded_state(states), dtype=torch.float32, device=device))\n",
    "        policies = torch.softmax(policies, axis=1).cpu().numpy()\n",
    "        policies = (1 - self.args['dirichlet_epsilon']) * policies + self.args['dirichlet_epsilon'] \\\n",
    "        * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size, size=policies.shape[0])\n",
    "        #for the current root state we are in in every game get its policy\n",
    "        valid_moves = np.array([self.game.get_valid_moves(state) for state in states])\n",
    "        #print(\"len(states) = \", len(states))\n",
    "        #print(\"len(spGames) = \", len(spGames))\n",
    "        for i in range(len(states)):\n",
    "            spGames[i].root = Node(states[i],self.args,i,player,self.game) #we create a new root node for each game and set its visit count to 1\n",
    "            policy = policies[i] * valid_moves[i] #we multiply the policy by the valid moves to get the valid policy\n",
    "            policy /= np.sum(policy) #we normalize thestates,spGames policy to get the probabilities of each action\n",
    "            spGames[i].root.expand(policy) # we only add the dirichlet noise at the root node\n",
    "\n",
    "\n",
    "        for _ in range(self.args[\"MCTS_iterations\"]):\n",
    "            for i in range(len(spGames)):\n",
    "                spGames[i].node = None\n",
    "                leaf_node = spGames[i].root\n",
    "                while leaf_node.is_fully_expanded():\n",
    "                    leaf_node, action_taken_by_opponent = leaf_node.choose_best_child() \n",
    "                #we go down the tree until we reach a leaf of the root node of the spgame\n",
    "                value, is_terminal = self.game.get_value_and_terminated(leaf_node.get_state(), leaf_node.transition_action)\n",
    "                value = self.game.get_opponent_value(value) \n",
    "                if is_terminal:\n",
    "                    leaf_node.set_value(value)\n",
    "                    leaf_node.backpropagate(value)\n",
    "                    spGames[i].node = None\n",
    "                else: \n",
    "                    spGames[i].node = leaf_node\n",
    "                #if we achieved a terminal state we backpropagate and this spgame is done if not we set spg.node to the leaf node to indicate that the game isnt over\n",
    "\n",
    "            not_finished_games =  [spGame for spGame in spGames if spGame.node != None]\n",
    "            #we get the not over games\n",
    "            if not_finished_games  == []:\n",
    "                continue \n",
    "            rollout_states = torch.tensor(\n",
    "                np.array([self.game.get_encoded_state(spGame.node.get_state()) for spGame in not_finished_games]),\n",
    "                dtype=torch.float32,\n",
    "                device=device  \n",
    "            )\n",
    "\n",
    "            policies, values = self.model.forward(rollout_states)\n",
    "            policies = torch.softmax(policies, axis=1).cpu().numpy()\n",
    "            values = values.cpu().numpy()\n",
    "            for i in range(len(not_finished_games)):\n",
    "                valid_moves  = self.game.get_valid_moves(not_finished_games[i].node.get_state())\n",
    "                true_policy = policies[i] * valid_moves\n",
    "                true_policy /= np.sum(true_policy) \n",
    "                value = values[i]\n",
    "                not_finished_games[i].node.set_value(value)\n",
    "                not_finished_games[i].node.backpropagate(value)\n",
    "                not_finished_games[i].node.expand(true_policy)\n",
    "            \n",
    "\n",
    "         \n",
    "         \n",
    "         \n",
    "         \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_vs_random(model, game, num_games=10):\n",
    "    \n",
    "    wins, draws, losses = 0, 0, 0\n",
    "\n",
    "    for _ in range(num_games):\n",
    "        state = game.get_initial_state()\n",
    "        player = 1\n",
    "        while True:\n",
    "            if player == 1:\n",
    "                # Model's turn\n",
    "                encoded = torch.tensor(game.get_encoded_state(state), dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                with torch.no_grad():\n",
    "                    policy_logits, _ = model(encoded)\n",
    "                    policy = torch.softmax(policy_logits, dim=1).cpu().numpy()[0]\n",
    "                    valid_moves = game.get_valid_moves(state)\n",
    "                    policy *= valid_moves\n",
    "                    if policy.sum() == 0:\n",
    "                        policy = valid_moves / valid_moves.sum()\n",
    "                    else:\n",
    "                        policy /= policy.sum()\n",
    "                    action = np.random.choice(game.action_size, p=policy)\n",
    "\n",
    "            else:\n",
    "                # Random agent's turn\n",
    "                valid_moves = game.get_valid_moves(state)\n",
    "                action = np.random.choice(np.flatnonzero(valid_moves))\n",
    "\n",
    "            state = game.get_next_state(state, action, player)\n",
    "            value, is_terminal = game.get_value_and_terminated(state, action)\n",
    "            if is_terminal:\n",
    "                if value == 1:\n",
    "                    if player == 1:\n",
    "                        wins += 1\n",
    "                    else:\n",
    "                        losses += 1\n",
    "                else:\n",
    "                    draws += 1\n",
    "                break\n",
    "            player = game.get_opponent(player)\n",
    "\n",
    "    total = wins + draws + losses\n",
    "    print(f\"Model vs Random — Wins: {wins}, Draws: {draws}, Losses: {losses} ({total} games)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AlphaZero():\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self,game,args,model,optimizer):\n",
    "        self.game = game\n",
    "        self.model = model\n",
    "        self.args = args \n",
    "        self.replay_buffer = [] #we store the historical data for the neural network valeus to be corrected here \n",
    "        self.current_game_tree =  SPG(game,args)\n",
    "        self.optimizer = optimizer\n",
    "        self.MCTS = MCTSParallel(self.game,self.args,self.model)\n",
    "\n",
    "    def game_of_SelfPlay(self,spGames): \n",
    "            \n",
    "            player  = 1\n",
    "            action = None\n",
    "            \n",
    "            self.model.eval()\n",
    "            to_remove = []\n",
    "            while len(spGames) > 0: \n",
    "                states = np.stack([spg.state for spg in spGames])\n",
    "                neutral_states = self.game.change_perspective(states, player)\n",
    "                self.MCTS.search(neutral_states,spGames,player)\n",
    "                \n",
    "                \n",
    "                for i in range(len(spGames))[::-1]:\n",
    "                    spg = spGames[i]\n",
    "                      \n",
    "                    single_policy = np.zeros(self.game.action_size)\n",
    "                    for child,action in spg.root.get_children():\n",
    "                        single_policy[action] = child.nb_visits\n",
    "                    single_policy /= np.sum(single_policy)\n",
    "\n",
    "                    #print(f\"we are appending state to memory for game {i}: \\n\",state)\n",
    "                    #print(f\"spGames[{i}].memory:\\n \",spGames[i].memory)\n",
    "                    #print(\"state, true_policy, player: \",state, true_policy, player)\n",
    "                    \n",
    "                    \n",
    "\n",
    "                    true_policy = single_policy\n",
    "                    spGames[i].memory.append((spg.root.state,true_policy,player))\n",
    "\n",
    "                    #With high temperature: probabilities become more uniform\n",
    "                    #With low temperature: differences get exaggerated, making the highest probability action even more likely\n",
    "                    action = np.random.choice(len(single_policy), p=single_policy)\n",
    "                    spg.state = self.game.get_next_state(spg.state, action, player)\n",
    "                    \n",
    "                    game_value, is_terminal = self.game.get_value_and_terminated(spg.state, action)\n",
    "                    #once an spg is finished we store the data that we stored in spg.memory in the replay buffer with the value of the end game and we remove the spg from the list\n",
    "                    #no need to keep track of the node tree since every state is already stored inside the spg memory \n",
    "                    if is_terminal:\n",
    "                         #seems like we are either putting 1 or 0 as the value of the position instead of a non discrete value\n",
    "                         # Over time, the value target for a position becomes an average of many discrete outcomes, which can be a non-discrete float\n",
    "\n",
    "                        \n",
    "                        for hist_state,true_policy,hist_player in spg.memory:\n",
    "\n",
    "                            hist_outcome = game_value if hist_player == player else self.game.get_opponent_value(game_value)\n",
    "                            print(\"\\n[Replay Buffer Debug]\")\n",
    "                            print(f\"Final game_value: {game_value}\")\n",
    "                            print(f\"Final player to move: {player}\")\n",
    "                            print(f\"Hist player: {hist_player}\")\n",
    "                            print(f\"Flipped value stored: {hist_outcome}\")\n",
    "                            print(f\"State:\\n{hist_state}\")\n",
    "                            print(f\"True policy:\\n{true_policy}\")\n",
    "\n",
    "                            self.replay_buffer.append((\n",
    "                            self.game.get_encoded_state(hist_state),\n",
    "                            true_policy,\n",
    "                            hist_outcome))\n",
    "                        del spGames[i]\n",
    "                       \n",
    "                player = self.game.get_opponent(player)\n",
    "\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "            random.shuffle(self.replay_buffer)\n",
    "            for j in trange(0, len(self.replay_buffer),self.args[\"batch_size\"], desc=\"Training\", leave=False):\n",
    "                batch = self.replay_buffer[j:min(j+self.args[\"batch_size\"], len(self.replay_buffer))]\n",
    "                states, true_policies, values = zip(*batch)\n",
    "                states, true_policies, values = np.array(states), np.array(true_policies), np.array(values)\n",
    "                states = torch.tensor(states, dtype=torch.float32, device=device)\n",
    "                policy_targets = torch.tensor(true_policies, dtype=torch.float32, device=device)\n",
    "                value_targets = torch.tensor(values, dtype=torch.float32, device=device).view(-1, 1)\n",
    "                policy_logits, value_logits = self.model.forward(states)\n",
    "                print(\"\\n=== Training Sample ===\")\n",
    "                \n",
    "                print(f\"Target Value: {value_targets[0].item():.4f}\")\n",
    "                print(f\"State:\\n{states[0].cpu()}\")\n",
    "                print(f\"Policy Target:\\n{policy_targets[0].cpu()}\")\n",
    "                print(f\"Policy Logits:\\n{policy_logits[0].cpu()}\")\n",
    "                policy_loss = F.cross_entropy(policy_logits,policy_targets)\n",
    "                #F.kl_div is better for soft class loss calculations \n",
    "                \"\"\"policy_loss = F.kl_div(\n",
    "                    F.log_softmax(policy_logits, dim=1),\n",
    "                    policy_targets,\n",
    "                    reduction='batchmean'\n",
    "                )\"\"\"\n",
    "\n",
    "                value_loss = F.mse_loss(value_logits,value_targets)\n",
    "                loss = policy_loss + value_loss\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                print(f\"value_loss : {value_loss}, policy_loss : {policy_loss}, loss : {loss}\")\n",
    "\n",
    "\n",
    "    def learn(self):   \n",
    "        if self.args[\"load_model\"] == True:\n",
    "            self.model.load_state_dict(torch.load(f\"model_{self.game}.pt\"))\n",
    "            self.optimizer.load_state_dict(torch.load(f\"optimizer_{self.game}.pt\"))   \n",
    "        for  i in range(self.args['iterations']):\n",
    "            for i in trange(self.args['num_parallel_selfplays']):\n",
    "                spGames = [SPG(self.game,self.args) for h in range(self.args[\"parallel_games\"])]     \n",
    "                self.game_of_SelfPlay(spGames)\n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train()\n",
    "            self.replay_buffer = []\n",
    "            if i % self.args[\"save_frequency\"] == 0:\n",
    "                torch.save(self.model.state_dict(), f\"model_{self.game}.pt\")\n",
    "                torch.save(self.optimizer.state_dict(), f\"optimizer_{self.game}.pt\")\n",
    "\n",
    "            \n",
    "        \n",
    "\n",
    "    def plot_metrics(self):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot losses\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(self.iterations, self.value_losses, label='Value Loss')\n",
    "        plt.plot(self.iterations, self.policy_losses, label='Policy Loss')\n",
    "        plt.title('AlphaZero Training Losses')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Plot total loss\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(self.iterations, self.total_losses, label='Total Loss', color='green')\n",
    "        plt.title('AlphaZero Total Loss')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'alphazero_metrics_{len(self.iterations)}.png')\n",
    "        plt.close()\n",
    "        \n",
    "    def free_memory(self):\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "class SPG():\n",
    "    \n",
    "    def __init__(self,game,args):\n",
    "        self.game = game\n",
    "        self.memory = []\n",
    "        self.state = game.get_initial_state()\n",
    "        self.root = None\n",
    "        self.node = None\n",
    "        \n",
    "\n",
    "\n",
    "         \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy shape: torch.Size([1, 7])\n",
      "Value prediction: 0.31358757615089417\n",
      "Policy probs (sum): 0.9999999403953552\n",
      "Top move prob: 0.2724999785423279\n"
     ]
    }
   ],
   "source": [
    "#Testing the ResNet model\n",
    "\n",
    "game = ConnectFour()\n",
    "model = ResNet(game, 9, 128, 3).to(device)\n",
    "\n",
    "# Generate a dummy board\n",
    "state = game.get_initial_state()\n",
    "encoded = torch.tensor(game.get_encoded_state(state), dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "# Run a forward pass\n",
    "policy_logits, value = model(encoded)\n",
    "policy_probs = torch.softmax(policy_logits, dim=1)\n",
    "\n",
    "print(\"Policy shape:\", policy_probs.shape)\n",
    "print(\"Value prediction:\", value.item())\n",
    "print(\"Policy probs (sum):\", policy_probs.sum().item())\n",
    "print(\"Top move prob:\", policy_probs.max().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing a round of SelfPlay\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'MCTS_iterations': 20,\n",
    "    'num_parallel_selfplays': 5,\n",
    "    'iterations': 5,\n",
    "    'parallel_games': 30,\n",
    "    'batch_size': 64,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3,\n",
    "    'temperature': 1.25,\n",
    "    'load_model': False,\n",
    "    'save_frequency': 5,\n",
    "    'num_epochs': 4,\n",
    "}\n",
    "alphaZero = AlphaZero(game,args,model,optimizer)\n",
    "alphaZero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model vs Random — Wins: 5, Draws: 0, Losses: 5 (10 games)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "evaluate_vs_random(model, game, num_games=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "#model.load_state_dict(torch.load(\"model_ConnectFour.pt\"))\n",
    "\n",
    "import pygame\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Game settings\n",
    "ROWS, COLS = 6, 7\n",
    "SQUARESIZE = 100\n",
    "RADIUS = int(SQUARESIZE / 2 - 5)\n",
    "WIDTH, HEIGHT = COLS * SQUARESIZE, (ROWS + 1) * SQUARESIZE\n",
    "\n",
    "# Colors\n",
    "BLACK = (0, 0, 0)\n",
    "BLUE = (0, 0, 255)\n",
    "RED = (255, 0, 0)\n",
    "YELLOW = (255, 255, 0)\n",
    "\n",
    "def draw_board(screen, board):\n",
    "    screen.fill(BLACK)\n",
    "    for c in range(COLS):\n",
    "        for r in range(ROWS):\n",
    "            pygame.draw.rect(screen, BLUE, (c*SQUARESIZE, (r+1)*SQUARESIZE, SQUARESIZE, SQUARESIZE))\n",
    "            piece = board[r][c]\n",
    "            color = BLACK\n",
    "            if piece == 1:\n",
    "                color = RED\n",
    "            elif piece == -1:\n",
    "                color = YELLOW\n",
    "            pygame.draw.circle(screen, color, (int(c*SQUARESIZE + SQUARESIZE/2), int((r+1)*SQUARESIZE + SQUARESIZE/2)), RADIUS)\n",
    "    pygame.display.update()\n",
    "\n",
    "def get_column_from_mouse(x):\n",
    "    return int(x / SQUARESIZE)\n",
    "\n",
    "def get_row_to_drop(board, col):\n",
    "    for r in range(ROWS-1, -1, -1):\n",
    "        if board[r][col] == 0:\n",
    "            return r\n",
    "    return None\n",
    "\n",
    "def play_against_model_pygame(model, game, device, play_as=1):\n",
    "    pygame.init()\n",
    "    screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "    font = pygame.font.SysFont(\"monospace\", 50)\n",
    "    pygame.display.set_caption(\"Connect Four - Play vs Model\")\n",
    "\n",
    "    board = np.zeros((ROWS, COLS), dtype=int)\n",
    "    state = game.get_initial_state()\n",
    "    player = 1\n",
    "    game_over = False\n",
    "\n",
    "    draw_board(screen, board)\n",
    "\n",
    "    while not game_over:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                return\n",
    "\n",
    "            if player == play_as and event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                col = get_column_from_mouse(event.pos[0])\n",
    "                valid_moves = game.get_valid_moves(state)\n",
    "                if valid_moves[col] == 1:\n",
    "                    row = get_row_to_drop(board, col)\n",
    "                    board[row][col] = player\n",
    "                    state = game.get_next_state(state, col, player)\n",
    "                    value, is_terminal = game.get_value_and_terminated(state, col)\n",
    "                    draw_board(screen, board)\n",
    "\n",
    "                    if is_terminal:\n",
    "                        game_over = True\n",
    "                        text = \"You win!\" if value == 1 else \"Draw!\" if value == 0 else \"Model wins!\"\n",
    "                        label = font.render(text, True, RED if value == 1 else YELLOW)\n",
    "                        screen.blit(label, (40, 10))\n",
    "                        pygame.display.update()\n",
    "                        time.sleep(3)\n",
    "                        break\n",
    "\n",
    "                    player = game.get_opponent(player)\n",
    "\n",
    "        if player != play_as and not game_over:\n",
    "            pygame.time.wait(500)\n",
    "            encoded = torch.tensor(game.get_encoded_state(state), dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                policy_logits, _ = model(encoded)\n",
    "                policy = torch.softmax(policy_logits, dim=1).cpu().numpy()[0]\n",
    "                valid_moves = game.get_valid_moves(state)\n",
    "                policy *= valid_moves\n",
    "                if policy.sum() == 0:\n",
    "                    policy = valid_moves / valid_moves.sum()\n",
    "                else:\n",
    "                    policy /= policy.sum()\n",
    "                action = np.random.choice(game.action_size, p=policy)\n",
    "\n",
    "            row = get_row_to_drop(board, action)\n",
    "            board[row][action] = player\n",
    "            state = game.get_next_state(state, action, player)\n",
    "            value, is_terminal = game.get_value_and_terminated(state, action)\n",
    "            draw_board(screen, board)\n",
    "\n",
    "            if is_terminal:\n",
    "                game_over = True\n",
    "                text = \"Model wins!\" if value == 1 else \"Draw!\" if value == 0 else \"You win!\"\n",
    "                label = font.render(text, True, YELLOW if value == 1 else RED)\n",
    "                screen.blit(label, (40, 10))\n",
    "                pygame.display.update()\n",
    "                time.sleep(3)\n",
    "                break\n",
    "\n",
    "            player = game.get_opponent(player)\n",
    "\n",
    "    pygame.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'play_against_model_pygame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mplay_against_model_pygame\u001b[49m(model, game, device, play_as\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'play_against_model_pygame' is not defined"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "play_against_model_pygame(model, game, device, play_as=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
